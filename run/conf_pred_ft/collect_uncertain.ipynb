{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0, 1, 2, 3\"\n",
    "from kilt import retrieval\n",
    "from kilt import kilt_utils as utils\n",
    "import tasks\n",
    "import utils\n",
    "from rouge_score import rouge_scorer\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import opensource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-multiset-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\", device_map='cuda')\n",
    "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\", device_map='cuda')\n",
    "wiki = load_dataset(path='wiki_dpr', name='psgs_w100.multiset.compressed', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='trivia'\n",
    "dataset_dpr = tasks.RQA_dpr(task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic = False\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "                                        use_stemmer=True)\n",
    "if semantic:\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "    # setup semantic model\n",
    "    semantic_tokenizer = \\\n",
    "        AutoTokenizer.from_pretrained(\"microsoft/deberta-large-mnli\")\n",
    "    semantic_model = \\\n",
    "        AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/deberta-large-mnli\"\n",
    "        ).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(dataset_dpr.elements))\n",
    "random.shuffle(indices)\n",
    "cal_indices = indices[:int(len(indices) * 0.5)]\n",
    "test_indices = indices[int(len(indices) * 0.5):]\n",
    "\n",
    "elements = dataset_dpr.elements\n",
    "query = [element['question'] for element in elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-multiset-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n",
    "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = q_encoder(**q_tokenizer(query, return_tensors=\"pt\", padding=True))\n",
    "question_embedding = question_embedding[0].numpy()\n",
    "scores, retrieved_examples = wiki.get_nearest_examples_batch('embeddings', question_embedding, k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup opensource model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, pipeline, tokenizer = opensource.setup_openmodel(model='lmsys/vicuna-7b-v1.5-16k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def write_list(a_list, file_name):\n",
    "    # store list in binary file so 'wb' mode\n",
    "    with open(file_name, 'wb') as fp:\n",
    "        pickle.dump(a_list, fp)\n",
    "#         print('Done writing list into a binary file')\n",
    "def read_list(file_name):\n",
    "    # for reading also binary mode is important\n",
    "    with open(file_name, 'rb') as fp:\n",
    "        n_list = pickle.load(fp)\n",
    "        return n_list\n",
    "\n",
    "def save_results(task):\n",
    "    # save retrieved_scores to a pickle file\n",
    "    write_list(retrieved_scores, f'uncertain_retrieved_scores_{task}.p')\n",
    "    # save retrieved_true_scores to a pickle file\n",
    "    write_list(retrieved_true_scores, f'uncertain_retrieved_true_scores_{task}.p')\n",
    "    # save queries to a pickle file\n",
    "    write_list(queries, f'uncertain_queries_{task}.p')\n",
    "    # save answers to a pickle file\n",
    "    write_list(answers, f'uncertain_answers_{task}.p')\n",
    "    # save passages to a pickle file\n",
    "    write_list(passages, f'uncertain_passages_{task}.p')\n",
    "    # save opensource_true_scores to a pickle file\n",
    "    write_list(opensource_true_scores, f'uncertain_opensource_true_scores_{task}.p')\n",
    "    # save opensource_texts to a pickle file\n",
    "#     write_list(opensource_texts, f'opensource_texts_{task}.p')\n",
    "    # save opensource_answers to a pickle file\n",
    "    write_list(opensource_answers, f'uncertain_opensource_answers_{task}.p')\n",
    "    # save opensource_semantics to a picle file\n",
    "    write_list(opensource_semantics, f'uncertain_opensource_semantics_{task}.p')\n",
    "    # save occurances to a pickle file\n",
    "    write_list(occurances, f'uncertain_occurances_{task}.p')\n",
    "    # save semantic_ids to a pickle file\n",
    "    write_list(semantic_ids, f'uncertain_semantic_ids_{task}.p')\n",
    "    # save probs to a picle file\n",
    "    write_list(probs, f'uncertain_probs_{task}.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "queries = []\n",
    "answers = []\n",
    "passages = []\n",
    "retrieved_scores = []\n",
    "retrieved_true_scores = []\n",
    "opensource_true_scores = []\n",
    "opensource_texts = []\n",
    "opensource_answers = []\n",
    "opensource_semantics = []\n",
    "semantic_probs = []\n",
    "feasibilities = []\n",
    "occurances = []\n",
    "semantic_ids = []\n",
    "probs = []\n",
    "with torch.no_grad():\n",
    "    for idx, (element, score, retrieved) in enumerate(zip(elements, scores, retrieved_examples)):\n",
    "        print(f'{idx}, {task}', file=open(f'uncertaint_{task}.txt', 'a'))\n",
    "        feasible = False\n",
    "        if idx % 10 == 0:\n",
    "            print(idx)\n",
    "            save_results(task)\n",
    "        query, answer, passage_id, passage_title, passage_text = \\\n",
    "            utils.dataset_info(element, dataset=task)\n",
    "        if len(passage_id) == 0:\n",
    "            continue\n",
    "        retrieved_ids, retrieved_texts, retrieved_title, true_score = \\\n",
    "            utils.retrieved_info(score, retrieved, passage_id[0])\n",
    "        if len(true_score) == 0:\n",
    "            continue\n",
    "        \n",
    "        prompt = utils.get_prompt_template(query, passage_text[0], task='Natural Questions')\n",
    "        sequences = opensource.ask_openmodel(prompt, pipeline, tokenizer, return_sequences=30)\n",
    "        generated_texts = []\n",
    "        for seq in sequences:\n",
    "            generated_texts.append(seq['generated_text'][len(prompt):].strip())\n",
    "        semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "            utils.clustering(generated_texts, prompt, scorer=scorer)\n",
    "        true_scores, matched_answer, semantics = utils.processing_answers(\n",
    "            semantic_set_ids, semantic_probs, \n",
    "            item_occurance, answer, scorer,\n",
    "            threshold=0.3\n",
    "        )\n",
    "        if len(true_scores) == 0:\n",
    "            retrieved_scores.append(score)\n",
    "            retrieved_true_scores.append(true_score)\n",
    "            queries.append(query)\n",
    "            answers.append(answer)\n",
    "            passages.append(passage_text)\n",
    "            opensource_true_scores.append(true_scores)\n",
    "            opensource_answers.append(generated_texts)\n",
    "            occurances.append(item_occurance)\n",
    "            semantic_ids.append(semantic_set_ids)\n",
    "            probs.append(semantic_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
