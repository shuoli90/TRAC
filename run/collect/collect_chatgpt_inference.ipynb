{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0, 1, 2, 3\"\n",
    "from utils import tasks\n",
    "from utils import utils\n",
    "from rouge_score import rouge_scorer\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-multiset-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.weight', 'ctx_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "Found cached dataset wiki_dpr (/home/lishuo1/.cache/huggingface/datasets/wiki_dpr/psgs_w100.multiset.compressed/0.0.0/74d4bff38a7c18a9498fafef864a8ba7129e27cb8d71b22f5e14d84cb17edd54)\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\", device_map='cuda')\n",
    "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\", device_map='cuda')\n",
    "wiki = load_dataset(path='wiki_dpr', name='psgs_w100.multiset.compressed', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='nq'\n",
    "dataset_dpr = tasks.RQA_dpr(task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic = False\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "                                        use_stemmer=True)\n",
    "if semantic:\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "    # setup semantic model\n",
    "    semantic_tokenizer = \\\n",
    "        AutoTokenizer.from_pretrained(\"microsoft/deberta-large-mnli\")\n",
    "    semantic_model = \\\n",
    "        AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/deberta-large-mnli\"\n",
    "        ).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(dataset_dpr.elements))\n",
    "random.shuffle(indices)\n",
    "cal_indices = indices[:int(len(indices) * 0.5)]\n",
    "test_indices = indices[int(len(indices) * 0.5):]\n",
    "\n",
    "elements = dataset_dpr.elements\n",
    "query = [element['question'] for element in elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-multiset-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n",
    "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = q_encoder(**q_tokenizer(query, return_tensors=\"pt\", padding=True))\n",
    "question_embedding = question_embedding[0].numpy()\n",
    "retrieved_scores, retrieved_examples = wiki.get_nearest_examples_batch('embeddings', question_embedding, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def write_list(a_list, file_name):\n",
    "    # store list in binary file so 'wb' mode\n",
    "    with open(file_name, 'wb') as fp:\n",
    "        pickle.dump(a_list, fp)\n",
    "#         print('Done writing list into a binary file')\n",
    "def read_list(file_name):\n",
    "    # for reading also binary mode is important\n",
    "    with open(file_name, 'rb') as fp:\n",
    "        n_list = pickle.load(fp)\n",
    "        return n_list\n",
    "\n",
    "def save_results(task):\n",
    "    # save retrieved_scores to a pickle file\n",
    "    write_list(retrieved_scores, f'chatgpt_retrieved_scores_{task}.p')\n",
    "    # save retrieved_true_scores to a pickle file\n",
    "    write_list(retrieved_true_scores, f'chatgpt_retrieved_true_scores_{task}.p')\n",
    "    # save queries to a pickle file\n",
    "    write_list(queries, f'chatgpt_queries_{task}.p')\n",
    "    # save answers to a pickle file\n",
    "    write_list(answers, f'chatgpt_true_answers_{task}.p')\n",
    "    # save passages to a pickle file\n",
    "    write_list(passages, f'chatgpt_passages_{task}.p')\n",
    "    # save chatgpt_true_scores to a pickle file\n",
    "    write_list(chatgpt_true_scores, f'chatgpt_true_scores_{task}.p')\n",
    "    # save chatgpt_texts to a pickle file\n",
    "#     write_list(chatgpt_texts, f'chatgpt_texts_{task}.p')\n",
    "    # save chatgpt_answers to a pickle file\n",
    "    write_list(chatgpt_answers, f'chatgpt_answers_{task}.p')\n",
    "    # save chatgpt_semantics to a picle file\n",
    "    write_list(chatgpt_semantics, f'chatgpt_semantics_{task}.p')\n",
    "    # save feasibilities to a pickle file\n",
    "    write_list(feasibilities, f'chatgpt_feasibilities_{task}.p')\n",
    "    # save occurances to a pickle file\n",
    "    write_list(occurances, f'chatgpt_occurances_{task}.p')\n",
    "    # save semantic_ids to a pickle file\n",
    "    write_list(semantic_ids, f'chatgpt_semantic_ids_{task}.p')\n",
    "    # save probs to a picle file\n",
    "    write_list(probs, f'chatgpt_probs_{task}.p')\n",
    "    \n",
    "    write_list(retrieved_scores_unc, f'chatgpt_retrieved_scores_unc_{task}.p')\n",
    "    write_list(retrieved_true_scores_unc, f'chatgpt_retrieved_true_scores_unc_{task}.p')\n",
    "    write_list(queries_unc, f'chatgpt_queries_unc_{task}.p')\n",
    "    write_list(answers_unc, f'chatgpt_answers_unc_{task}.p')\n",
    "    write_list(passages_unc, f'chatgpt_passages_unc_{task}.p')\n",
    "    write_list(chatgpt_true_scores_unc, f'chatgpt_true_scores_unc_{task}.p')\n",
    "    write_list(chatgpt_answers_unc, f'chatgpt_answers_unc_{task}.p')\n",
    "    write_list(occurances_unc, f'chatgpt_occurances_unc_{task}.p')\n",
    "    write_list(semantic_ids_unc, f'chatgpt_semantic_ids_unc_{task}.p')\n",
    "    write_list(probs_unc, f'chatgpt_probs_unc_{task}.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_results(task):\n",
    "    retrieved_scores = read_list(f'chatgpt_retrieved_scores_{task}.p')\n",
    "    retrieved_true_scores = read_list(f'chatgpt_retrieved_true_scores_{task}.p')\n",
    "    queries = read_list(f'chatgpt_queries_{task}.p')\n",
    "    answers = read_list(f'chatgpt_answers_{task}.p')\n",
    "    chatgpt_true_scores = read_list(f'chatgpt_true_scores_{task}.p')\n",
    "    chatgpt_answers = read_list(f'chatgpt_answers_{task}.p')\n",
    "    chatgpt_passages = read_list(f'chatgpt_passages_{task}.p')\n",
    "    chatgpt_semantics = read_list(f'chatgpt_semantics_{task}.p')\n",
    "    chatgpt_occurances = read_list(f'chatgpt_occurances_{task}.p')\n",
    "    chatgpt_semantic_ids = read_list(f'chatgpt_semantic_ids_{task}.p')\n",
    "    chatgpt_probs = read_list(f'chatgpt_probs_{task}.p')\n",
    "    \n",
    "    retrieved_scores_unc = read_list(f'chatgpt_retrieved_scores_unc_{task}.p')\n",
    "    retrieved_true_scores_unc = read_list(f'chatgpt_retrieved_true_scores_unc_{task}.p')\n",
    "    queries_unc = read_list(f'chatgpt_queries_unc_{task}.p')\n",
    "    answers_unc = read_list(f'chatgpt_answers_unc_{task}.p')\n",
    "    passages_unc = read_list(f'chatgpt_passages_unc_{task}.p')\n",
    "    chatgpt_true_scores_unc = read_list(f'chatgpt_true_scores_unc_{task}.p')\n",
    "    chatgpt_answers_unc = read_list(f'chatgpt_answers_unc_{task}.p')\n",
    "    chatgpt_occurances_unc = read_list(f'chatgpt_occurances_unc_{task}.p')\n",
    "    chatgpt_semantic_ids_unc = read_list(f'chatgpt_semantic_ids_unc_{task}.p')\n",
    "    chatgpt_probs_unc = read_list(f'chatgpt_probs_unc_{task}.p')\n",
    "    \n",
    "    return retrieved_scores, retrieved_true_scores, queries, answers, chatgpt_true_scores, chatgpt_answers, chatgpt_passages, chatgpt_semantics, chatgpt_occurances, chatgpt_semantic_ids, chatgpt_probs, retrieved_scores_unc, retrieved_true_scores_unc, queries_unc, answers_unc, passages_unc, chatgpt_true_scores_unc, chatgpt_answers_unc, chatgpt_occurances_unc, chatgpt_semantic_ids_unc, chatgpt_probs_unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.setup_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chatgpt_results(task, dir='../collected_data'):\n",
    "    retrieved_scores = read_list(os.path.join(dir, f'chatgpt_retrieved_scores_{task}.p'))\n",
    "    retrieved_true_scores = read_list(os.path.join(dir, f'chatgpt_retrieved_true_scores_{task}.p'))\n",
    "    queries = read_list(os.path.join(dir, f'chatgpt_queries_{task}.p'))\n",
    "    answers = read_list(os.path.join(dir, f'chatgpt_answers_{task}.p'))\n",
    "    chatgpt_true_scores = read_list(os.path.join(dir, f'chatgpt_true_scores_{task}.p'))\n",
    "    chatgpt_answers = read_list(os.path.join(dir, f'chatgpt_answers_{task}.p'))\n",
    "    chatgpt_passages = read_list(os.path.join(dir, f'chatgpt_passages_{task}.p'))\n",
    "    chatgpt_semantics = read_list(os.path.join(dir, f'chatgpt_semantics_{task}.p'))\n",
    "    chatgpt_occurances = read_list(os.path.join(dir, f'chatgpt_occurances_{task}.p'))\n",
    "    chatgpt_semantic_ids = read_list(os.path.join(dir, f'chatgpt_semantic_ids_{task}.p'))\n",
    "    chatgpt_probs = read_list(os.path.join(dir, f'chatgpt_probs_{task}.p'))\n",
    "    retrieved_scores_unc = read_list(os.path.join(dir, f'chatgpt_retrieved_scores_unc_{task}.p'))\n",
    "    retrieved_true_scores_unc = read_list(os.path.join(dir, f'chatgpt_retrieved_true_scores_unc_{task}.p'))\n",
    "    queries_unc = read_list(os.path.join(dir, f'chatgpt_queries_unc_{task}.p'))\n",
    "    answers_unc = read_list(os.path.join(dir, f'chatgpt_answers_unc_{task}.p'))\n",
    "    passages_unc = read_list(os.path.join(dir, f'chatgpt_passages_unc_{task}.p'))\n",
    "    chatgpt_true_scores_unc = read_list(os.path.join(dir, f'chatgpt_true_scores_unc_{task}.p'))\n",
    "    chatgpt_answers_unc = read_list(os.path.join(dir, f'chatgpt_answers_unc_{task}.p'))\n",
    "    chatgpt_occurances_unc = read_list(os.path.join(dir, f'chatgpt_occurances_unc_{task}.p'))\n",
    "    chatgpt_semantic_ids_unc = read_list(os.path.join(dir, f'chatgpt_semantic_ids_unc_{task}.p'))\n",
    "    chatgpt_probs_unc = read_list(os.path.join(dir, f'chatgpt_probs_unc_{task}.p'))\n",
    "    return retrieved_scores, retrieved_true_scores, queries, answers, \\\n",
    "        chatgpt_true_scores, chatgpt_answers, chatgpt_passages, \\\n",
    "        chatgpt_semantics, chatgpt_occurances, chatgpt_semantic_ids, \\\n",
    "        chatgpt_probs, retrieved_scores_unc, retrieved_true_scores_unc, \\\n",
    "        queries_unc, answers_unc, passages_unc, chatgpt_true_scores_unc, \\\n",
    "        chatgpt_answers_unc, chatgpt_occurances_unc, \\\n",
    "        chatgpt_semantic_ids_unc, chatgpt_probs_unc\n",
    "\n",
    "def coverage(\n",
    "        retrieved_true_scores_list, opensource_true_scores_list,\n",
    "        retrieved_thr, qa_thr):\n",
    "\n",
    "    includes = []\n",
    "    for idx, (retrieved_true_score, opensource_true_score) in enumerate(zip(retrieved_true_scores_list, opensource_true_scores_list)):\n",
    "#         if idx > 20:\n",
    "        opensource_true_score = np.max(opensource_true_score)\n",
    "        include = True if (retrieved_true_score >= retrieved_thr and \n",
    "                        opensource_true_score >= qa_thr) \\\n",
    "                    else False\n",
    "        includes.append(include)\n",
    "    return includes\n",
    "\n",
    "retrieved_scores, retrieved_true_scores, queries, answers, chatgpt_true_scores, chatgpt_answers, chatgpt_passages, chatgpt_semantics, chatgpt_occurances, chatgpt_semantic_ids, chatgpt_probs, retrieved_scores_unc, retrieved_true_scores_unc, queries_unc, answers_unc, passages_unc, chatgpt_true_scores_unc, chatgpt_answers_unc, chatgpt_occurances_unc, chatgpt_semantic_ids_unc, chatgpt_probs_unc = \\\n",
    "            read_chatgpt_results('nq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual components\n",
      "retrieval coverage 0.9003322259136213\n",
      "qa coverage 0.9102990033222591\n",
      "test retrieval coverage 0.8980099502487562\n",
      "test qa coverage 0.8955223880597015\n",
      "End-to-end coverage 0.8084577114427861\n"
     ]
    }
   ],
   "source": [
    "if task == 'bio':\n",
    "        task='bio'\n",
    "        all_queries, contexts, gold_answers = tasks.bio_dpr(task=task).load_dataset()\n",
    "else:\n",
    "    dataset_dpr = tasks.RQA_dpr(task)\n",
    "    elements = dataset_dpr.elements\n",
    "    all_queries = [element['question'] for element in elements]\n",
    "answers = []\n",
    "if task == 'bio':\n",
    "    for query in queries:\n",
    "        idx = all_queries.index(query)\n",
    "        answers.append(gold_answers[idx])\n",
    "else:\n",
    "    for query in queries:\n",
    "        idx = all_queries.index(query)\n",
    "        answers.append(elements[idx]['answers'])\n",
    "\n",
    "indices = np.arange(len(retrieved_true_scores))\n",
    "random.shuffle(indices)\n",
    "cal_first_indices = indices[:int(len(indices) * 0.3)]\n",
    "cal_second_indices = indices[int(len(indices) * 0.3) : int(len(indices) * 0.6)]\n",
    "test_indices = indices[int(len(indices) * 0.6):]\n",
    "\n",
    "cal_first_retrieved_true_scores = utils.split(retrieved_true_scores, cal_first_indices)\n",
    "cal_second_retrieved_true_scores = utils.split(retrieved_true_scores, cal_second_indices)\n",
    "test_retrieved_true_scores = utils.split(retrieved_true_scores, test_indices)\n",
    "\n",
    "cal_first_chatgpt_true_scores = utils.split(chatgpt_true_scores, cal_first_indices)\n",
    "cal_second_chatgpt_true_scores = utils.split(chatgpt_true_scores, cal_second_indices)\n",
    "test_chatgpt_true_scores = utils.split(chatgpt_true_scores, test_indices)\n",
    "\n",
    "cal_first_retrieved_scores = utils.split(retrieved_scores, cal_first_indices)\n",
    "cal_second_retrieved_scores = utils.split(retrieved_scores, cal_second_indices)\n",
    "test_retrieved_scores = utils.split(retrieved_scores, test_indices)\n",
    "\n",
    "cal_first_chatgpt_occurances = utils.split(chatgpt_occurances, cal_first_indices)\n",
    "cal_second_chatgpt_occurances = utils.split(chatgpt_occurances, cal_second_indices)\n",
    "test_chatgpt_occurances = utils.split(chatgpt_occurances, test_indices)\n",
    "\n",
    "cal_first_chatgpt_semantic_ids = utils.split(chatgpt_semantic_ids, cal_first_indices)\n",
    "cal_second_chatgpt_semantic_ids = utils.split(chatgpt_semantic_ids, cal_second_indices)\n",
    "test_chatgpt_semantic_ids = utils.split(chatgpt_semantic_ids, test_indices)\n",
    "\n",
    "cal_first_chatgpt_probs = utils.split(chatgpt_probs, cal_first_indices)\n",
    "cal_second_chatgpt_probs = utils.split(chatgpt_probs, cal_second_indices)\n",
    "test_chatgpt_probs = utils.split(chatgpt_probs, test_indices)\n",
    "\n",
    "cal_first_queries = utils.split(queries, cal_first_indices)\n",
    "cal_second_queries = utils.split(queries, cal_second_indices)\n",
    "test_queries = utils.split(queries, test_indices)\n",
    "\n",
    "cal_first_chatgpt_answers = utils.split(chatgpt_answers, cal_first_indices)\n",
    "cal_second_chatgpt_answers = utils.split(chatgpt_answers, cal_second_indices)\n",
    "test_chatgpt_answers = utils.split(chatgpt_answers, test_indices)\n",
    "\n",
    "cal_first_answers = utils.split(answers, cal_first_indices)\n",
    "cal_second_answers = utils.split(answers, cal_second_indices)\n",
    "test_answers = utils.split(answers, test_indices)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "                                    use_stemmer=True)\n",
    "\n",
    "print(\"Individual components\")\n",
    "if task == 'bio':\n",
    "    # cal_first_retrieved_true_scores = [np.max(scores) for scores in cal_first_retrieved_true_scores]\n",
    "    tmp = []\n",
    "    for scores in cal_first_retrieved_true_scores:\n",
    "        tmp.extend(scores)\n",
    "    cal_first_retrieved_true_scores = tmp\n",
    "retrieved_thr = utils.compute_threshold(cal_first_retrieved_true_scores, alpha=alpha/2)\n",
    "cal_first_scores = []\n",
    "for scores in cal_first_chatgpt_true_scores:\n",
    "    cal_first_scores.append(np.max(scores))\n",
    "chatgpt_qa_thr = utils.compute_threshold(cal_first_scores, alpha=alpha/2)\n",
    "\n",
    "if task == 'bio':\n",
    "    # cal_second_retrieved_true_scores = [np.max(scores) for scores in cal_second_retrieved_true_scores]\n",
    "    tmp = []\n",
    "    for scores in cal_second_retrieved_true_scores:\n",
    "        tmp.extend(scores)\n",
    "    cal_second_retrieved_true_scores = tmp\n",
    "retrieved_coverage = np.mean(np.array(cal_second_retrieved_true_scores) >= retrieved_thr)\n",
    "cal_second_scores = []\n",
    "for scores in cal_second_chatgpt_true_scores:\n",
    "    cal_second_scores.append(np.max(scores))\n",
    "qa_coverage = np.mean(np.array(cal_second_scores) >= chatgpt_qa_thr)\n",
    "print('retrieval coverage', retrieved_coverage)\n",
    "print('qa coverage', qa_coverage)\n",
    "\n",
    "if task == 'bio':\n",
    "    # test_retrieved_true_scores = [np.max(scores) for scores in test_retrieved_true_scores]\n",
    "    tmp = []\n",
    "    for scores in test_retrieved_true_scores:\n",
    "        tmp.extend(scores)\n",
    "    test_retrieved_true_scores = tmp\n",
    "retrieved_coverage = np.mean(np.array(test_retrieved_true_scores) >= retrieved_thr)\n",
    "test_scores = []\n",
    "for scores in test_chatgpt_true_scores:\n",
    "    test_scores.append(np.max(scores))\n",
    "qa_coverage = np.mean(np.array(test_scores) >= chatgpt_qa_thr)\n",
    "print('test retrieval coverage', retrieved_coverage)\n",
    "print('test qa coverage', qa_coverage)\n",
    "\n",
    "coverages = coverage(\n",
    "    test_retrieved_true_scores, \n",
    "    test_chatgpt_true_scores,\n",
    "    retrieved_thr,\n",
    "    chatgpt_qa_thr\n",
    "    )\n",
    "print('End-to-end coverage', np.mean(coverages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 10\n",
    "length = len(test_retrieved_scores)\n",
    "lens = np.linspace(0, length, kernel+1)\n",
    "test_retrieved_scores_list = [test_retrieved_scores[int(lens[i]):int(lens[i+1])] for i in range(kernel)]\n",
    "test_chatgpt_semantic_ids_list = [test_chatgpt_semantic_ids[int(lens[i]):int(lens[i+1])] for i in range(kernel)]\n",
    "test_chatgpt_probs_list = [test_chatgpt_probs[int(lens[i]):int(lens[i+1])] for i in range(kernel)]\n",
    "test_answers_list = [test_answers[int(lens[i]):int(lens[i+1])] for i in range(kernel)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_chatgpt(prompts,\n",
    "                   model=\"gpt-3.5-turbo-0613\",\n",
    "                   temperature=1.0,\n",
    "                   max_token=20,\n",
    "                   n_answers=5):\n",
    "\n",
    "    messages = [{\"role\": \"user\",\n",
    "                 \"content\": prompt} for prompt in prompts]\n",
    "    response = utils.chatcompletions_with_backoff(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,  # this is the degree of randomness of the model's output\n",
    "        n=n_answers  # how many different answers to return\n",
    "    )\n",
    "\n",
    "    choices = [choice.message['content'].strip()\n",
    "               for choice\n",
    "               in response.choices]\n",
    "    input_token_counts = response.usage['prompt_tokens']\n",
    "    output_token_counts = response.usage['completion_tokens']\n",
    "\n",
    "    # return response.choices[0].message[\"content\"]\n",
    "    return choices, input_token_counts, output_token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "total_times = []\n",
    "single_times = []\n",
    "multiple_times = []\n",
    "cluster_times = []\n",
    "for idx, (element, score, retrieved) in enumerate(zip(elements, retrieved_scores, retrieved_examples)):\n",
    "    if idx >= 20:\n",
    "        break\n",
    "    semantics = []  \n",
    "    time_count = [] \n",
    "    total_start = time.time()\n",
    "    prompts = []\n",
    "    for s, context in zip(score, retrieved):\n",
    "        # start = time.time()\n",
    "        if s < retrieved_thr:\n",
    "            continue\n",
    "        prompt = utils.get_prompt_template(query, context, task='Natural Questions')\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    single_start = time.time()\n",
    "    sequences = ask_chatgpt(prompts, n_answers=1)[0]\n",
    "    single_end = time.time()\n",
    "    single_times.append(single_end - single_start)\n",
    "    \n",
    "    multiple_start = time.time()\n",
    "    all_sequences = ask_chatgpt(prompts, n_answers=30)[0]\n",
    "    multiple_end = time.time()\n",
    "    multiple_times.append(multiple_end - multiple_start)\n",
    "\n",
    "    cluster_start = time.time()\n",
    "    for sequences in all_sequences:\n",
    "        semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.clustering(sequences, prompt, scorer=scorer)\n",
    "        for predicted_answer in semantic_set_ids.keys():\n",
    "            concept_id = semantic_set_ids[predicted_answer]\n",
    "            prob = semantic_probs[concept_id]\n",
    "            if prob >= chatgpt_qa_thr:\n",
    "                semantics.append(predicted_answer)\n",
    "\n",
    "    semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.clustering(semantics, \"\", scorer=scorer)\n",
    "    total_end = time.time()\n",
    "    cluster_end = time.time()\n",
    "    total_time = total_end - total_start\n",
    "    total_times.append(total_time)\n",
    "    cluster_time = cluster_end - cluster_start\n",
    "    cluster_times.append(cluster_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time 2.1573036909103394\n",
      "single time 1.0639436721801758\n",
      "multiple time 1.8580217838287354\n",
      "clustering time 0.2991437554359436\n"
     ]
    }
   ],
   "source": [
    "print('total time', (np.sum(total_times) - np.sum(single_times)) / len(total_times))\n",
    "print('single time', np.mean(single_times))\n",
    "print('multiple time', np.mean(multiple_times))\n",
    "print('clustering time', np.mean(cluster_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
