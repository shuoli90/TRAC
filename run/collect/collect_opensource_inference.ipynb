{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3,5\"\n",
    "from utils import tasks\n",
    "from utils import utils\n",
    "from rouge_score import rouge_scorer\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from utils import opensource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-multiset-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.weight', 'ctx_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "Found cached dataset wiki_dpr (/home/lishuo1/.cache/huggingface/datasets/wiki_dpr/psgs_w100.multiset.compressed/0.0.0/74d4bff38a7c18a9498fafef864a8ba7129e27cb8d71b22f5e14d84cb17edd54)\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\", device_map='cuda')\n",
    "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\", device_map='cuda')\n",
    "wiki = load_dataset(path='wiki_dpr', name='psgs_w100.multiset.compressed', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='nq'\n",
    "dataset_dpr = tasks.RQA_dpr(task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic = False\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "                                        use_stemmer=True)\n",
    "if semantic:\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "    # setup semantic model\n",
    "    semantic_tokenizer = \\\n",
    "        AutoTokenizer.from_pretrained(\"microsoft/deberta-large-mnli\")\n",
    "    semantic_model = \\\n",
    "        AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/deberta-large-mnli\"\n",
    "        ).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(dataset_dpr.elements))\n",
    "random.shuffle(indices)\n",
    "cal_indices = indices[:int(len(indices) * 0.5)]\n",
    "test_indices = indices[int(len(indices) * 0.5):]\n",
    "\n",
    "elements = dataset_dpr.elements\n",
    "query = [element['question'] for element in elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-multiset-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.weight', 'question_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n",
    "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = q_encoder(**q_tokenizer(query, return_tensors=\"pt\", padding=True))\n",
    "question_embedding = question_embedding[0].numpy()\n",
    "relevance, retrieved_examples = wiki.get_nearest_examples_batch('embeddings', question_embedding, k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup opensource model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57ba58a462f4092b1fdfe6081427a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lishuo1/anaconda3/envs/glu/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/lishuo1/anaconda3/envs/glu/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "# from trl import SFTTrainer\n",
    "import tasks\n",
    "import utils\n",
    "# from trl import DataCollatorForCompletionOnlyLM\n",
    "from datasets import Dataset\n",
    "import json\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "# Fine-tuned model name\n",
    "new_model = f\"../finetuned_models/llama-2-7b-shuo-{task}\"\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model_new = PeftModel.from_pretrained(base_model, new_model)\n",
    "# model_new = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "pipe_new = pipeline(\n",
    "    task=\"text-generation\", \n",
    "    model=model_new, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=300,\n",
    "    return_full_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def write_list(a_list, file_name):\n",
    "    # store list in binary file so 'wb' mode\n",
    "    with open(file_name, 'wb') as fp:\n",
    "        pickle.dump(a_list, fp)\n",
    "#         print('Done writing list into a binary file')\n",
    "def read_list(file_name):\n",
    "    # for reading also binary mode is important\n",
    "    with open(file_name, 'rb') as fp:\n",
    "        n_list = pickle.load(fp)\n",
    "        return n_list\n",
    "\n",
    "def coverage(\n",
    "        retrieved_true_scores_list, opensource_true_scores_list,\n",
    "        retrieved_thr, qa_thr):\n",
    "\n",
    "    includes = []\n",
    "    for idx, (retrieved_true_score, opensource_true_score) in enumerate(zip(retrieved_true_scores_list, opensource_true_scores_list)):\n",
    "#         if idx > 20:\n",
    "        opensource_true_score = np.max(opensource_true_score)\n",
    "        include = True if (retrieved_true_score >= retrieved_thr and \n",
    "                        opensource_true_score >= qa_thr) \\\n",
    "                    else False\n",
    "        includes.append(include)\n",
    "    return includes\n",
    "\n",
    "def save_results(task):\n",
    "    # save retrieved_scores to a pickle file\n",
    "    write_list(retrieved_scores, f'retrieved_scores_{task}_new.p')\n",
    "    # save retrieved_true_scores to a pickle file\n",
    "    write_list(retrieved_true_scores, f'retrieved_true_scores_{task}_new.p')\n",
    "    # save queries to a pickle file\n",
    "    write_list(queries, f'queries_{task}_new.p')\n",
    "    # save answers to a pickle file\n",
    "    write_list(answers, f'answers_{task}_new.p')\n",
    "    # save passages to a pickle file\n",
    "    write_list(passages, f'passages_{task}_new.p')\n",
    "    # save opensource_true_scores to a pickle file\n",
    "    write_list(opensource_true_scores, f'opensource_true_scores_{task}_new.p')\n",
    "    # save opensource_texts to a pickle file\n",
    "#     write_list(opensource_texts, f'opensource_texts_{task}.p')\n",
    "    # save opensource_answers to a pickle file\n",
    "    write_list(opensource_answers, f'opensource_answers_{task}_new.p')\n",
    "    # save opensource_semantics to a picle file\n",
    "    write_list(opensource_semantics, f'opensource_semantics_{task}_new.p')\n",
    "    # save feasibilities to a pickle file\n",
    "    write_list(feasibilities, f'feasibilities_{task}_new.p')\n",
    "    # save occurances to a pickle file\n",
    "    write_list(occurances, f'occurances_{task}_new.p')\n",
    "    # save semantic_ids to a pickle file\n",
    "    write_list(semantic_ids, f'semantic_ids_{task}_new.p')\n",
    "    # save probs to a picle file\n",
    "    write_list(probs, f'probs_{task}_new.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_results(task, end=1000, dir='../collected_data'):\n",
    "    retrieved_scores = read_list(os.path.join(dir, f'retrieved_scores_{task}.p'))[:end]\n",
    "    retrieved_true_scores = read_list(os.path.join(dir, f'retrieved_true_scores_{task}.p'))[:end]\n",
    "    queries = read_list(os.path.join(dir, f'queries_{task}.p'))[:end]\n",
    "    answers = read_list(os.path.join(dir, f'answers_{task}.p'))[:end]\n",
    "    opensource_true_scores = read_list(os.path.join(dir, f'opensource_true_scores_{task}.p'))[:end]\n",
    "    opensource_answers = read_list(os.path.join(dir, f'opensource_answers_{task}.p'))[:end]\n",
    "    opensource_semantics = read_list(os.path.join(dir, f'opensource_semantics_{task}.p'))[:end]\n",
    "    opensource_occurances = read_list(os.path.join(dir, f'occurances_{task}.p'))[:end]\n",
    "    opensource_semantic_ids = read_list(os.path.join(dir, f'semantic_ids_{task}.p'))[:end]\n",
    "    opensource_probs = read_list(os.path.join(dir, f'probs_{task}.p'))[:end]\n",
    "    \n",
    "    return retrieved_scores, retrieved_true_scores, \\\n",
    "           queries, answers, \\\n",
    "           opensource_true_scores, opensource_answers, \\\n",
    "           opensource_occurances, opensource_semantic_ids, opensource_probs\n",
    "\n",
    "retrieved_scores, retrieved_true_scores, queries, answers, opensource_true_scores, opensource_answers, opensource_occurances, opensource_semantic_ids, opensource_probs = \\\n",
    "        read_results(task, end=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(retrieved_true_scores))\n",
    "random.shuffle(indices)\n",
    "cal_first_indices = indices[:int(len(indices) * 0.3)]\n",
    "cal_second_indices = indices[int(len(indices) * 0.3) : int(len(indices) * 0.6)]\n",
    "test_indices = indices[int(len(indices) * 0.6):]\n",
    "\n",
    "# indices = np.arange(1000)\n",
    "indices = np.arange(len(queries))\n",
    "random.shuffle(indices)\n",
    "cal_first_indices = indices[:int(len(indices) * 0.3)]\n",
    "cal_second_indices = indices[int(len(indices) * 0.3) : int(len(indices) * 0.6)]\n",
    "test_indices = indices[int(len(indices) * 0.6):]\n",
    "# test_indices = indices[int(len(indices) * 0.3):]\n",
    "\n",
    "# indices = np.arange(1000)\n",
    "indices = np.arange(len(queries))\n",
    "random.shuffle(indices)\n",
    "cal_first_indices = indices[:int(len(indices) * 0.3)]\n",
    "cal_second_indices = indices[int(len(indices) * 0.3) : int(len(indices) * 0.6)]\n",
    "test_indices = indices[int(len(indices) * 0.6):]\n",
    "# test_indices = indices[int(len(indices) * 0.3):]\n",
    "\n",
    "cal_first_retrieved_true_scores = utils.split(retrieved_true_scores, cal_first_indices)\n",
    "cal_second_retrieved_true_scores = utils.split(retrieved_true_scores, cal_second_indices)\n",
    "test_retrieved_true_scores = utils.split(retrieved_true_scores, test_indices)\n",
    "cal_first_opensource_true_scores = utils.split(opensource_true_scores, cal_first_indices)\n",
    "cal_second_opensource_true_scores = utils.split(opensource_true_scores, cal_second_indices)\n",
    "test_opensource_true_scores = utils.split(opensource_true_scores, test_indices)\n",
    "cal_first_retrieved_scores = utils.split(retrieved_scores, cal_first_indices)\n",
    "cal_second_retrieved_scores = utils.split(retrieved_scores, cal_second_indices)\n",
    "test_retrieved_scores = utils.split(retrieved_scores, test_indices)\n",
    "cal_first_opensource_occurances = utils.split(opensource_occurances, cal_first_indices)\n",
    "cal_second_opensource_occurances = utils.split(opensource_occurances, cal_second_indices)\n",
    "test_opensource_occurances = utils.split(opensource_occurances, test_indices)\n",
    "cal_first_opensource_semantic_ids = utils.split(opensource_semantic_ids, cal_first_indices)\n",
    "cal_second_opensource_semantic_ids = utils.split(opensource_semantic_ids, cal_second_indices)\n",
    "test_opensource_semantic_ids = utils.split(opensource_semantic_ids, test_indices)\n",
    "cal_first_queries = utils.split(queries, cal_first_indices)\n",
    "cal_second_queries = utils.split(queries, cal_second_indices)\n",
    "test_queries = utils.split(queries, test_indices)\n",
    "cal_first_opensource_answers = utils.split(opensource_answers, cal_first_indices)\n",
    "cal_second_opensource_answers = utils.split(opensource_answers, cal_second_indices)\n",
    "test_opensource_answers = utils.split(opensource_answers, test_indices)\n",
    "cal_first_answers = utils.split(answers, cal_first_indices)\n",
    "cal_second_answers = utils.split(answers, cal_second_indices)\n",
    "test_answers = utils.split(answers, test_indices)\n",
    "cal_first_opensource_probs = utils.split(opensource_probs, cal_first_indices)\n",
    "cal_second_opensource_probs = utils.split(opensource_probs, cal_second_indices)\n",
    "test_opensource_probs = utils.split(opensource_probs, test_indices)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "                                    use_stemmer=True)\n",
    "\n",
    "print(\"Individual components\")\n",
    "retrieved_thr = utils.compute_threshold(cal_first_retrieved_true_scores, alpha=alpha/2)\n",
    "cal_first_scores = []\n",
    "for scores in cal_first_opensource_true_scores:\n",
    "    cal_first_scores.append(np.max(scores))\n",
    "opensource_qa_thr = utils.compute_threshold(cal_first_scores, alpha=alpha/2)\n",
    "\n",
    "retrieved_coverage = np.mean(np.array(cal_second_retrieved_true_scores) >= retrieved_thr)\n",
    "cal_second_scores = []\n",
    "for scores in cal_second_opensource_true_scores:\n",
    "    cal_second_scores.append(np.max(scores))\n",
    "qa_coverage = np.mean(np.array(cal_second_scores) >= opensource_qa_thr)\n",
    "print('retrieval coverage', retrieved_coverage)\n",
    "print('qa coverage', qa_coverage)\n",
    "\n",
    "retrieved_coverage = np.mean(np.array(test_retrieved_true_scores) >= retrieved_thr)\n",
    "test_scores = []\n",
    "for scores in test_opensource_true_scores:\n",
    "    test_scores.append(np.max(scores))\n",
    "qa_coverage = np.mean(np.array(test_scores) >= opensource_qa_thr)\n",
    "print('test retrieval coverage', retrieved_coverage)\n",
    "print('test qa coverage', qa_coverage)\n",
    "\n",
    "coverages = coverage(\n",
    "    test_retrieved_true_scores,\n",
    "    test_opensource_true_scores,\n",
    "    retrieved_thr,\n",
    "    opensource_qa_thr\n",
    "    )\n",
    "print('End-to-end coverage', np.mean(coverages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "single_times = []\n",
    "multiple_times = []\n",
    "clustering_times = []\n",
    "total_times = []\n",
    "\n",
    "for idx, (element, score, retrieved) in enumerate(zip(elements, retrieved_scores, retrieved_examples)):\n",
    "    query_start = time.time()\n",
    "    if idx >= 50:\n",
    "        break\n",
    "    semantics = []  \n",
    "    time_count = [] \n",
    "    query = element['question']\n",
    "    cluster_time = 0.0\n",
    "\n",
    "    prompts = []\n",
    "    # for s, context in zip(score, retrieved['text']):\n",
    "    #     if s < retrieved_thr:\n",
    "    #         continue\n",
    "    #     prompt = utils.get_prompt_template(query, context, task='Natural Questions')\n",
    "    #     prompts.append(prompt)\n",
    "\n",
    "    # single_start = time.time()\n",
    "    # sequences = opensource.ask_openmodel(prompts, pipe_new, tokenizer, return_sequences=1)\n",
    "    # single_end = time.time()\n",
    "    # single_times.append(single_end-single_start)\n",
    "\n",
    "    # multiple_start = time.time()\n",
    "    # all_sequences = opensource.ask_openmodel(prompts, pipe_new, tokenizer, return_sequences=30)\n",
    "    # multiple_end = time.time()\n",
    "    # multiple_times.append(multiple_end-multiple_start)\n",
    "\n",
    "    # for sequences in all_sequences:\n",
    "\n",
    "    \n",
    "    for s, context in zip(score, retrieved['text']):\n",
    "        if s < retrieved_thr:\n",
    "            continue\n",
    "        prompt = utils.get_prompt_template(query, context, task='Natural Questions')\n",
    "\n",
    "        single_start = time.time()\n",
    "        sequences = opensource.ask_openmodel(prompt, pipe_new, tokenizer, return_sequences=1)\n",
    "        single_end = time.time() \n",
    "        single_times.append(single_end-single_start)\n",
    "\n",
    "        multiple_start = time.time()\n",
    "        sequences = opensource.ask_openmodel(prompt, pipe_new, tokenizer, return_sequences=30)\n",
    "        multiple_end = time.time()\n",
    "        multiple_times.append(multiple_end-multiple_start)\n",
    "\n",
    "        clustering_start = time.time()\n",
    "        generated_texts = []\n",
    "        for seq in sequences:\n",
    "            tmp = seq['generated_text']\n",
    "            idx = np.char.find(tmp, \"~!~\", start=0, end=None)\n",
    "            tmp = tmp[:idx].strip()\n",
    "            generated_texts.append(tmp)\n",
    "    \n",
    "        semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.clustering(generated_texts, prompt, scorer=scorer)\n",
    "        for predicted_answer in semantic_set_ids.keys():\n",
    "            concept_id = semantic_set_ids[predicted_answer]\n",
    "            prob = semantic_probs[concept_id]\n",
    "            if prob >= opensource_qa_thr:\n",
    "                semantics.append(predicted_answer)\n",
    "        clustering_end = time.time()\n",
    "        cluster_time += clustering_end-clustering_start\n",
    "\n",
    "    start = time.time()\n",
    "    semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.clustering(semantics, \"\", scorer=scorer)\n",
    "    end = time.time()\n",
    "    cluster_time += end-start\n",
    "    clustering_times.append(cluster_time)\n",
    "    query_end = time.time()\n",
    "    total_times.append(query_end-query_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time 82.52295764923096\n",
      "single time 2.906776759935462\n",
      "multiple time 5.2724736084413655\n",
      "clustering time 0.060995445251464844\n"
     ]
    }
   ],
   "source": [
    "print('total time', (np.sum(total_times) - np.sum(single_times)) / 50)\n",
    "print('single time', np.mean(single_times))\n",
    "print('multiple time', np.mean(multiple_times))\n",
    "print('clustering time', np.mean(clustering_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "single_times_new = []\n",
    "multiple_times_new = []\n",
    "clustering_times_new = []\n",
    "total_times_new = []\n",
    "\n",
    "for idx, (element, score, retrieved) in enumerate(zip(elements, retrieved_scores, retrieved_examples)):\n",
    "    query_start = time.time()\n",
    "    if idx >= 20:\n",
    "        break\n",
    "    semantics = []  \n",
    "    time_count = [] \n",
    "    query = element['question']\n",
    "    cluster_time = 0.0\n",
    "\n",
    "    prompts = []\n",
    "    for s, context in zip(score, retrieved['text']):\n",
    "        if s < retrieved_thr:\n",
    "            continue\n",
    "        prompt = utils.get_prompt_template(query, context, task='Natural Questions')\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    single_start = time.time()\n",
    "    sequences = opensource.ask_openmodel(prompts, pipe_new, tokenizer, return_sequences=1)\n",
    "    single_end = time.time()\n",
    "    single_times_new.append(single_end-single_start)\n",
    "\n",
    "    multiple_start = time.time()\n",
    "    all_sequences = opensource.ask_openmodel(prompts, pipe_new, tokenizer, return_sequences=30)\n",
    "    multiple_end = time.time()\n",
    "    multiple_times_new.append(multiple_end-multiple_start)\n",
    "\n",
    "    for sequences in all_sequences:\n",
    "\n",
    "    \n",
    "    # for s, context in zip(score, retrieved['text']):\n",
    "    #     if s < retrieved_thr:\n",
    "    #         continue\n",
    "    #     prompt = utils.get_prompt_template(query, context, task='Natural Questions')\n",
    "\n",
    "    #     single_start = time.time()\n",
    "    #     sequences = opensource.ask_openmodel(prompt, pipe_new, tokenizer, return_sequences=1)\n",
    "    #     single_end = time.time() \n",
    "    #     single_times.append(single_end-single_start)\n",
    "\n",
    "    #     multiple_start = time.time()\n",
    "    #     sequences = opensource.ask_openmodel(prompt, pipe_new, tokenizer, return_sequences=30)\n",
    "    #     multiple_end = time.time()\n",
    "    #     multiple_times.append(multiple_end-multiple_start)\n",
    "\n",
    "        clustering_start = time.time()\n",
    "        generated_texts = []\n",
    "        for seq in sequences:\n",
    "            tmp = seq['generated_text']\n",
    "            idx = np.char.find(tmp, \"~!~\", start=0, end=None)\n",
    "            tmp = tmp[:idx].strip()\n",
    "            generated_texts.append(tmp)\n",
    "    \n",
    "        semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.clustering(generated_texts, prompt, scorer=scorer)\n",
    "        for predicted_answer in semantic_set_ids.keys():\n",
    "            concept_id = semantic_set_ids[predicted_answer]\n",
    "            prob = semantic_probs[concept_id]\n",
    "            if prob >= opensource_qa_thr:\n",
    "                semantics.append(predicted_answer)\n",
    "        clustering_end = time.time()\n",
    "        cluster_time += clustering_end-clustering_start\n",
    "\n",
    "    start = time.time()\n",
    "    semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.clustering(semantics, \"\", scorer=scorer)\n",
    "    end = time.time()\n",
    "    cluster_time += end-start\n",
    "    clustering_times_new.append(cluster_time)\n",
    "    query_end = time.time()\n",
    "    total_times_new.append(query_end-query_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time 1715.7902722358704\n",
      "single time 63.029936051368715\n",
      "multiple time 85.65073851346969\n",
      "clustering time 0.13865853548049928\n"
     ]
    }
   ],
   "source": [
    "print('total time', (np.sum(total_times_new) - np.sum(single_times_new)))\n",
    "print('single time', np.mean(single_times_new))\n",
    "print('multiple time', np.mean(multiple_times_new))\n",
    "print('clustering time', np.mean(clustering_times_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
